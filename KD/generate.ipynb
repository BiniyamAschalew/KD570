{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "transform_pipeline = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(checkpoint_path):\n",
    "    model = models.resnet18(pretrained=False)  # Example model, change according to your needs\n",
    "    # model.load_state_dict(torch.load(checkpoint_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    # random image tensor\n",
    "    image = torch.rand(3, 256, 256)\n",
    "    return transform_pipeline(image).unsqueeze(0)\n",
    "\n",
    "def process_activation(model, input_tensor, target_layer, iterations=30, lr=1.0):\n",
    "    layer_output = {}\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        layer_output[target_layer] = output\n",
    "\n",
    "    handle = getattr(model, target_layer).register_forward_hook(hook)\n",
    "    input_tensor.requires_grad = True\n",
    "\n",
    "    for i in range(iterations):\n",
    "        model.zero_grad()\n",
    "        _ = model(input_tensor)\n",
    "        activation = layer_output[target_layer].mean()\n",
    "        activation.backward()\n",
    "        input_tensor.data += lr * input_tensor.grad.data\n",
    "        input_tensor.grad.zero_()\n",
    "\n",
    "    handle.remove()\n",
    "    return input_tensor.detach()\n",
    "\n",
    "def deprocess_image(tensor):\n",
    "    tensor = tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    tensor = tensor.clamp(0, 1)\n",
    "    tensor = tensor.numpy().transpose(1, 2, 0)\n",
    "    return (tensor * 255).astype(np.uint8)\n",
    "\n",
    "def activation_maximization(checkpoint_path, image_path, target_layer):\n",
    "    model = load_model(checkpoint_path)\n",
    "    input_tensor = load_image(image_path)\n",
    "    optimized_tensor = process_activation(model, input_tensor, target_layer)\n",
    "    return deprocess_image(optimized_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bini/anaconda3/envs/augcon/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/bini/anaconda3/envs/augcon/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/augcon/lib/python3.9/site-packages/PIL/Image.py:3231\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3231\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   3232\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m target_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 5\u001b[0m resulting_image \u001b[38;5;241m=\u001b[39m \u001b[43mactivation_maximization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 41\u001b[0m, in \u001b[0;36mactivation_maximization\u001b[0;34m(checkpoint_path, image_path, target_layer)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mactivation_maximization\u001b[39m(checkpoint_path, image_path, target_layer):\n\u001b[1;32m     40\u001b[0m     model \u001b[38;5;241m=\u001b[39m load_model(checkpoint_path)\n\u001b[0;32m---> 41\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     optimized_tensor \u001b[38;5;241m=\u001b[39m process_activation(model, input_tensor, target_layer)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m deprocess_image(optimized_tensor)\n",
      "Cell \u001b[0;32mIn[47], line 8\u001b[0m, in \u001b[0;36mload_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_image\u001b[39m(image_path):\n\u001b[0;32m----> 8\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# random image tensor\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/augcon/lib/python3.9/site-packages/PIL/Image.py:3233\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3231\u001b[0m     fp\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   3232\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation):\n\u001b[0;32m-> 3233\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m())\n\u001b[1;32m   3234\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3236\u001b[0m prefix \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m16\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "# for usage\n",
    "checkpoint_path = 'trained_models/ResNet18_MNIST.pth'\n",
    "image_path = ''\n",
    "target_layer = 2\n",
    "resulting_image = activation_maximization(checkpoint_path, image_path, target_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating synthetic dataset using activation maximization for each output class of the given model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from models import load_model\n",
    "from utils import load_config\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class SyntheticGenerator():\n",
    "    def __init__(self, pretrained_model, data_config, device, logger, size_per_class, iterations=1000):\n",
    "\n",
    "        self.model = pretrained_model\n",
    "        self.size = size_per_class\n",
    "        self.device = device\n",
    "        \n",
    "        self.logger = logger\n",
    "        self.iterations = iterations\n",
    "        self.data_config = data_config\n",
    "\n",
    "        print(self.data_config[\"input_shape\"])\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(self.data_config[\"input_shape\"][1:]),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        \"\"\" convert the activation function of the final layer of the model from softmax to linear \"\"\"\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def generate_dataset(self,  batch_size = 64,output_dir = None):\n",
    "\n",
    "        data = []\n",
    "        data_labels = []\n",
    "        dataloader = None\n",
    "\n",
    "        for _ in range(self.size):\n",
    "            for label in range(self.data_config[\"num_classes\"]):\n",
    "\n",
    "                gen_img = self.generate_batch(label, batch_size)\n",
    "                data.append(gen_img)\n",
    "                data_labels.append(label)\n",
    "\n",
    "        # data = torch.stack(data) \n",
    "        # change the shape of the data from labels x batch x img_shape  to\n",
    "        data_labels = torch.tensor(data_labels)\n",
    "\n",
    "        dataloader = DataLoader(TensorDataset(data, data_labels), batch_size= batch_size, shuffle=True)\n",
    "\n",
    "        if output_dir:\n",
    "            print(\"Saving dataset to \", output_dir)\n",
    "            torch.save(dataloader, output_dir)\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "    def generate_batch(self, label, batch_size):\n",
    "\n",
    "        image = self.random_image(batch_size).to(self.device)\n",
    "        image.requires_grad = True\n",
    "\n",
    "        optimizer = optim.Adam([image], lr=0.01)\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = self.model(image)\n",
    "            loss = -output[0][label]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "    def random_image(self, batch_size = 64):\n",
    "\n",
    "        random_high, random_low = 180, 160\n",
    "\n",
    "        mean=torch.tensor([0.485]).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "        std=torch.tensor([0.229]).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "        random_high, random_low = 180, 160\n",
    "\n",
    "        random_image = torch.rand(batch_size, *self.data_config[\"input_shape\"])\n",
    "\n",
    "        print(f\"the shape of the generated image is {random_image.shape}\")\n",
    "        \n",
    "        random_image = (((random_high - random_low) * random_image + random_low)/255)\n",
    "        random_image = (random_image - mean) / std\n",
    "\n",
    "\n",
    "        return random_image\n",
    "\n",
    "def visualize_image(input_image: torch.Tensor):\n",
    "    \"\"\"given a torch tensor of shape [1, 28, 28] visualize an image of the MNIST dataset\"\"\"\n",
    "\n",
    "    input_image = input_image.cpu().detach().numpy().squeeze()\n",
    "    plt.imshow(input_image, cmap='gray')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from models import load_model\n",
    "from utils import load_config\n",
    "\n",
    "class SyntheticGenerator():\n",
    "    def __init__(self, pretrained_model, data_config, device, logger, size_per_class, iterations=1000):\n",
    "        self.model = pretrained_model\n",
    "        self.size = size_per_class\n",
    "        self.device = device\n",
    "        self.logger = logger\n",
    "        self.iterations = iterations\n",
    "        self.data_config = data_config\n",
    "\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Transformations, if necessary, adjust according to actual need\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(self.data_config[\"input_shape\"][1:]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485], std=[0.229])  # Adjust as per actual dataset mean/std\n",
    "        ])\n",
    "\n",
    "    def generate_dataset(self, batch_size=64, output_dir=None):\n",
    "        data = []\n",
    "        data_labels = []\n",
    "\n",
    "        for label in range(self.data_config[\"num_classes\"]):\n",
    "            for _ in range(self.size):\n",
    "                gen_img = self.generate_batch(label, batch_size)\n",
    "                data.append(gen_img)\n",
    "                data_labels.append(torch.full((batch_size,), label, dtype=torch.long))\n",
    "\n",
    "        data = torch.cat(data)\n",
    "        data_labels = torch.cat(data_labels)\n",
    "\n",
    "        dataloader = DataLoader(TensorDataset(data, data_labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        if output_dir:\n",
    "            print(\"Saving dataset to \", output_dir)\n",
    "            torch.save(dataloader, output_dir)\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "    def generate_batch(self, label, batch_size):\n",
    "        image = self.random_image(batch_size).to(self.device)\n",
    "        image.requires_grad = True\n",
    "        optimizer = optim.Adam([image], lr=0.01)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "            optimizer.zero_grad()\n",
    "            output = self.model(image)\n",
    "            class_loss = -output[:, label].sum()\n",
    "\n",
    "            # Regularization terms\n",
    "            l2_loss = 0.001 * torch.norm(image)\n",
    "            tv_loss = 0.001 * (torch.sum(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:])) +\n",
    "                               torch.sum(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :])))\n",
    "            \n",
    "            loss = class_loss + l2_loss + tv_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                image.clamp_(0, 1)  # Keep image values within valid range\n",
    "\n",
    "        return image.detach()\n",
    "\n",
    "    def random_image(self, batch_size=64):\n",
    "        # Initialize close to the dataset mean\n",
    "        random_image = torch.rand(batch_size, *self.data_config[\"input_shape\"], device=self.device)\n",
    "        random_image = random_image * 0.5 + 0.5  # Rescale to [0.5, 1.0]\n",
    "        return random_image\n",
    "\n",
    "def visualize_image(input_image: torch.Tensor):\n",
    "    \"\"\"given a torch tensor of shape [1, 28, 28] visualize an image of the MNIST dataset\"\"\"\n",
    "    input_image = input_image.cpu().detach().numpy().squeeze()\n",
    "    plt.imshow(input_image, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bini/anaconda3/envs/augcon/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/bini/anaconda3/envs/augcon/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model build as resnet18, pretrained: True, output channels: 10\n",
      "Loading model from ./trained_models/ResNet18_MNIST.pth\n",
      "Model loaded from ./trained_models/ResNet18_MNIST.pth\n",
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_dir = \"configs/model_configs/ResNet18_MNIST.yaml\"\n",
    "model_config = load_config(config_dir)\n",
    "model, trained = load_model(model_config, torch.device(\"cuda\"))\n",
    "\n",
    "if not trained:\n",
    "    raise ValueError(\"Model must be trained.\")\n",
    "\n",
    "dataset_dir = \"configs/dataset_configs/MNIST.yaml\"\n",
    "data_config = load_config(dataset_dir)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "logger = None\n",
    "size_per_class = 1\n",
    "\n",
    "generator = SyntheticGenerator(model, data_config, device, logger, size_per_class)\n",
    "\n",
    "print(\"model loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataloader = generator.generate_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, (batch_data, batch_label) in enumerate(sample_dataloader):\n",
    "\n",
    "    print(data.shape), print(label.shape)\n",
    "\n",
    "    rand_idx = 2\n",
    "    print(f\"the label is {label[rand_idx]}\")\n",
    "    visualize_image(data[rand_idx])\n",
    "\n",
    "    \n",
    "# data_l = data_l.unsqueeze(0)\n",
    "# output = model(data_l)\n",
    "# data_l.shape, print(output), output.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 28, 28])\n",
      "torch.Size([40960])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 36\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(TensorDataset(data, labels), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 36\u001b[0m sample_dataset \u001b[38;5;241m=\u001b[39m \u001b[43munpack_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m test_model(generator\u001b[38;5;241m.\u001b[39mmodel, sample_dataset, device, logger, model_config)\n",
      "Cell \u001b[0;32mIn[65], line 34\u001b[0m, in \u001b[0;36munpack_dataloader\u001b[0;34m(data_loader)\u001b[0m\n\u001b[1;32m     30\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(\u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/augcon/lib/python3.9/site-packages/torch/utils/data/dataset.py:192\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "from train import test_model\n",
    "from utils import get_logger\n",
    "\n",
    "logger = get_logger(\"logs\")\n",
    "\n",
    "def unpack_dataloader(data_loader):\n",
    "    \"\"\"change the shape of the data loader from 10 x 64 x img_shape to 640 x img_shape\"\"\"\n",
    "    \"return data loader\"\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for i, (d, l) in enumerate(data_loader):\n",
    "\n",
    "        data.append(d)\n",
    "        labels.append(l)\n",
    "\n",
    "    \"\"\" change data from List of tensors to a single tensor that is the concatenation of the tensors\"\"\"\n",
    "\n",
    "\n",
    "    data = torch.cat(data)\n",
    "    data = data.view(-1, *data.shape[2:])\n",
    "\n",
    "    print(data.shape)\n",
    "\n",
    "    \n",
    "    labels = torch.cat(labels)\n",
    "    # convert each value of labels to 64 values\n",
    "\n",
    "    labels = labels.repeat(64)\n",
    "    print(labels.shape)\n",
    "    \n",
    "\n",
    "    return DataLoader(TensorDataset(data, labels), batch_size=64, shuffle=True)\n",
    "\n",
    "sample_dataset = unpack_dataloader(sample_dataloader)\n",
    "test_model(generator.model, sample_dataset, device, logger, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model build as resnet18, pretrained: True, output channels: 10\n",
      "Loading model from ./trained_models/ResNet18_MNIST.pth\n",
      "Model loaded from ./trained_models/ResNet18_MNIST.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.0759,  -1.0002,   1.7754,   0.9550,  -3.2916,  -0.9564,  -3.4592,\n",
       "           2.7722,  -0.1488,  -1.5632],\n",
       "        [ -1.3871,  -2.2722,   0.2847,   0.4635,  -2.3006,  -2.1271,  -6.4316,\n",
       "           5.0379,  -3.9110,   2.2982],\n",
       "        [ -0.8139,  -1.3918,  -2.4930,  -1.0181,  -0.9492,   4.7967,   0.4596,\n",
       "          -1.5574,  -1.2047,  -1.6944],\n",
       "        [  1.4160,  -2.3102,   2.1292,  -1.3787,  -0.1696,  -2.1457,  -3.4218,\n",
       "           2.4685,  -2.2912,  -0.0589],\n",
       "        [  3.6905,   0.8090,   2.1424,  -1.3780,  -2.7229,  -2.9627,  -1.9441,\n",
       "          -0.3368,   0.0678,  -2.8549],\n",
       "        [ -0.9086,   2.0907,   3.5047,  -0.7963,   0.3548,  -3.3587,  -1.2824,\n",
       "           0.3582,  -2.2192,  -2.9956],\n",
       "        [ -2.8173,  -1.8814,  -5.3222,  -4.1239,   1.0306,   5.2410,   6.5485,\n",
       "          -7.3088,  -1.5296,  -0.4710],\n",
       "        [ -0.3334,   0.4692,   2.0075,   0.9493,  -2.0263,  -1.9701,  -1.5203,\n",
       "           0.5659,  -1.3945,  -2.2239],\n",
       "        [ -0.8927,   2.7841,  -3.3150,  -4.9243,  -0.1431,  -3.1204,  -4.5082,\n",
       "           2.3086,   2.8781,   1.5911],\n",
       "        [ -1.2090,   7.8385,  -7.3595, -26.3217,  24.5200, -14.7812,  -0.5119,\n",
       "          -0.5608, -12.1927,  -0.8389],\n",
       "        [ -2.3640,  -6.7392,  -9.2162,  -0.6337,  -6.1717,  10.5141,   2.6238,\n",
       "          -8.1571,   3.8011,   0.1238],\n",
       "        [ -0.7180,  -1.4659,  -3.2567,   0.7281,  -4.1335,   6.0822,  -0.8059,\n",
       "          -0.7654,  -1.8198,  -0.8569],\n",
       "        [ 20.4282, -21.6227,   2.1646,  -7.0865,  -4.8050,  -3.0203,  -2.6459,\n",
       "          -6.3653,  -2.5313,  -1.1828],\n",
       "        [ -1.3338,  -1.1659,  -2.7505,  -0.3022,  -2.8042,   3.0108,   1.3316,\n",
       "          -3.0645,   1.0774,  -0.4511],\n",
       "        [  0.4106,   2.6291,  -3.3817,  -2.3582,  -3.9272,  -3.1947,  -1.3343,\n",
       "          -1.8818,   5.4445,  -0.1943],\n",
       "        [  1.0236,   0.7614,  -2.4539,  -1.0240,  -3.2643,  -1.1402,  -0.5787,\n",
       "          -1.8345,   2.9951,  -0.0972],\n",
       "        [ -0.9023,  -0.2589,  -0.1633,  -2.2841,  -0.4185,  -1.9313,  -4.2652,\n",
       "           3.9721,  -2.6656,   1.7814],\n",
       "        [  0.1741,   2.7785,  -2.2197,  -4.8688,   0.5844,  -4.2134,  -5.0144,\n",
       "           5.2132,  -4.0177,   2.5274],\n",
       "        [  0.2466,  -0.9337,  -0.4172,  -1.2427,  -1.5909,  -0.9907,  -3.3458,\n",
       "           3.5801,  -2.0909,   0.5243],\n",
       "        [ -5.3660,  -8.3909,  -9.9935,  -1.4844,  -2.2673,  14.6697,  -7.0367,\n",
       "           3.0319,  -7.4692,   3.0663],\n",
       "        [  0.6567,  -0.0522,   1.0814,   2.0008,  -3.0088,  -1.2353,  -0.7151,\n",
       "          -0.7825,  -1.9308,  -2.5362],\n",
       "        [ -2.0634,  -2.0485,   2.2047,   0.9851,  -3.2952,  -0.2825,  -3.0231,\n",
       "           0.9333,   3.1565,  -2.3254],\n",
       "        [  0.5713,  -1.5894,  10.3272,  -1.2221,  -1.1674,  -6.0406,  -4.7970,\n",
       "           2.1551,  -2.9500,  -4.4821],\n",
       "        [  0.0388,   2.8643,  -2.3130,  -2.5226,  -1.1728,  -0.1364,   4.4905,\n",
       "          -4.7611,  -1.0795,  -2.9518],\n",
       "        [  3.5136,   6.5680,  -3.2192,  -3.2595,  -2.5952,  -4.4454,  -0.2783,\n",
       "          -2.0635,   0.7767,  -1.3895],\n",
       "        [ -1.9154,  -1.3805,  -4.1309,   2.0884,  -4.3234,   6.6608,  -0.1082,\n",
       "          -2.8108,  -1.7829,  -0.4166],\n",
       "        [  1.9389,  -1.8270,   6.3006,  -1.8482,  -4.2195,  -4.6249,   6.1385,\n",
       "          -7.6706,   1.6532,  -8.4010],\n",
       "        [-14.2944, -19.5397,  11.7236,  29.3347, -21.5288,  -5.6749, -34.8035,\n",
       "          12.1019,  -3.1844,   1.3428],\n",
       "        [  3.8002,  -4.4034,   1.9984,  -7.7766,   2.6712,  -4.0458,  13.1413,\n",
       "         -12.4140,  -2.3714,  -7.9269],\n",
       "        [ -0.1811,  -0.1185,   2.8547,  -1.0293,  -0.0868,  -2.2181,   2.2098,\n",
       "          -2.3776,  -1.2948,  -3.3362],\n",
       "        [  0.6105,   2.6650,  -1.5496,  -0.7208,  -3.4874,  -0.0996,   0.3452,\n",
       "          -1.9639,   1.2950,  -2.2693],\n",
       "        [  3.7693,  -2.3956,   1.3540,  -1.3029,  -1.0648,  -2.3459,  -1.9447,\n",
       "          -0.1913,  -1.5644,  -0.5014]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res18mnist, trained =\n",
    "# import os\n",
    "# print(os.path.curdir)\n",
    "model_config_dir = \"configs/model_configs/ResNet18_MNIST.yaml\"\n",
    "model_config1 = load_config(model_config_dir)\n",
    "\n",
    "\n",
    "image1 = torch.rand(32, 1, 28, 28)\n",
    "# image1.shape\n",
    "model1, t = load_model(model_config1, torch.device(\"cuda\"))\n",
    "\n",
    "model1(image1)\n",
    "\n",
    "# trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 3, 128, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "shape1 = [64,3,128,128]\n",
    "\n",
    "torch.rand(1, *shape1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "augcon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
